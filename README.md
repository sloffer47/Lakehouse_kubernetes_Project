🚀 Projet Lakehouse sur Kubernetes

Migration complète d’un pipeline Data Lakehouse vers Kubernetes (Kafka → Spark → Argo Workflows)

🧭 Sommaire

Présentation du projet

Architecture globale

Flux de données

Structure du projet

Étapes d’installation et d’exécution

Version A — Pipeline manuel (Kafka → Spark)

Version B — Pipeline orchestré avec Argo Workflows

Validation et résultats finaux

Technologies utilisées

Auteur

🧩 Présentation du projet

Ce projet a pour objectif de déployer une architecture Lakehouse complète sur Kubernetes, de bout en bout :

Ingestion temps réel avec Kafka

Transformation et stockage multi-couches (Bronze, Silver, Gold) avec Apache Spark

Stockage persistant via PersistentVolumeClaim (PVC)

Orchestration optionnelle avec Argo Workflows

L’ensemble est packagé sous forme de Jobs Kubernetes, permettant un déploiement reproductible sur n’importe quel cluster (Docker Desktop, Minikube, Kind, etc.).

🏗️ Architecture globale
┌─────────────────────────────────────────────┐
│         CLUSTER KUBERNETES                  │
├─────────────────────────────────────────────┤
│                                             │
│  1. KAFKA (Message Broker)                  │
│     ├── Kafka Broker (Port 9092)            │
│     └── Zookeeper (Port 2181)               │
│                                             │
│  2. PRODUCER (Générateur de données)        │
│     └── Job Kubernetes (Producer Java)      │
│                                             │
│  3. SPARK JOBS (Traitements ETL)            │
│     ├── Bronze : données brutes depuis Kafka│
│     ├── Silver : nettoyage et validation    │
│     └── Gold : agrégations statistiques     │
│                                             │
│  4. STORAGE (Lakehouse)                     │
│     └── PersistentVolume (10Gi)             │
│                                             │
│  5. ARGO WORKFLOWS (Orchestration) ⚙️       │
│     └── Exécution séquentielle automatique  │
│                                             │
└─────────────────────────────────────────────┘

📂 Structure du projet
lakehouse-kubernetes-project/
├── producer/                      # Producteur Kafka (Java)
│   ├── Producer.java
│   ├── Dockerfile
│   └── pom.xml
│
├── spark-jobs/                    # Jobs Spark (Scala)
│   ├── Bronze.scala
│   ├── Silver.scala
│   ├── Gold.scala
│   └── pom.xml
│
├── kubernetes/                    # Manifests Kubernetes
│   ├── namespace.yaml
│   ├── kafka-deployment.yaml
│   ├── topic-creation.yaml
│   ├── producer-job.yaml
│   ├── storage.yaml
│   ├── spark-jobs.yaml
│   └── spark-applications.yaml
│
├── argo/                          # (Optionnel) Orchestration
│   └── workflow.yaml
│
├── build.sh                       # Script d'installation rapide
├── setup.py                       # Configuration Python (si besoin)
└── README.md

⚙️ Étapes d’installation et d’exécution
🅰️ Version A — Pipeline manuel (Kafka → Spark)
1️⃣ Déployer Kafka
kubectl apply -f kubernetes/kafka-deployment.yaml
kubectl get pods -n lakehouse -w


Attendre que les pods soient Running :
zookeeper, kafka, et create-topics complété.

2️⃣ Vérifier le topic
kubectl exec -it kafka-xxx -n lakehouse -- kafka-topics --bootstrap-server localhost:9092 --list
# Résultat attendu : vehicles-events

3️⃣ Compiler et builder l’image du Producer
cd producer
mvn clean package
docker build -t producer:1.0 .

4️⃣ Déployer le Producer
kubectl apply -f kubernetes/producer-job.yaml
kubectl get jobs -n lakehouse -w
# Attendre : "producer-job 1/1 Completed"

5️⃣ Déployer le stockage
kubectl apply -f kubernetes/storage.yaml

6️⃣ Lancer les jobs Spark
kubectl apply -f kubernetes/spark-jobs.yaml
kubectl get jobs -n lakehouse -w


🟢 Attendu :

producer-job   Complete   1/1
bronze-job     Complete   1/1
silver-job     Complete   1/1
gold-job       Complete   1/1

🅱️ Version B — Pipeline orchestré avec Argo Workflows
1️⃣ Installer Argo Workflows
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update
helm install argo-workflows argo/argo-workflows \
  -n argo-workflows --create-namespace \
  --set server.serviceType=LoadBalancer

2️⃣ Créer le Workflow
kubectl apply -f argo/workflow.yaml -n argo-workflows

3️⃣ Vérifier le Workflow
kubectl get workflows -n argo-workflows -w


Les tâches doivent s’enchaîner dans cet ordre :

producer → bronze → silver → gold

📊 Validation et résultats finaux
🔍 Vérifier les résultats Spark Gold
kubectl exec -it gold-job-driver -n lakehouse -- bash
cd /data/gold/stats
ls

🔢 Lire les données avec Spark
spark-shell
scala> val df = spark.read.parquet("/data/gold/stats")
scala> df.show()


✅ Exemple de sortie :

+------+-------+-----------+-----------+
| type | total | avg_battery | max_lat |
+------+-------+-------------+----------+
| car  | 10    | 75.4        | 48.90    |
| bike | 10    | 67.1        | 48.92    |
| scoot| 10    | 82.3        | 48.95    |
+------+-------+-------------+----------+

🧰 Technologies utilisées
Composant	Rôle	Technologie
Messaging	Streaming temps réel	Apache Kafka
Ingestion	Génération d’événements	Producer Java
Traitement	ETL / Lakehouse	Apache Spark
Orchestration	Pipelines automatiques	Argo Workflows
Infrastructure	Déploiement distribué	Kubernetes
Stockage	Données persistantes	PVC / HostPath
👨‍💻 Auteur

Ton Nom / Promo / Email
Projet réalisé dans le cadre du module Cloud & Big Data Engineering